{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxev3rrvdTfl",
        "outputId": "991ca19a-ff4c-4298-e402-ad5bf210302e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 30 14:13:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKvFz8aGeLu4",
        "outputId": "f7b25d8d-88cf-42bd-e14c-890b2971c149"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = 'Hello, i am fine?'\n",
        "tokens = tokenizer(text, return_tensors = 'pt')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fztmjVWIeAQX",
        "outputId": "2b3e08cb-ea1c-42ad-eac8-b5dda872d2e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[15496,    11,  1312,   716,  3734,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Encode input\n",
        "input_ids = tokenizer.encode(\"india economy\", return_tensors='pt')\n",
        "\n",
        "# Generate text (note: 'max_length' was misspelled)\n",
        "output = model.generate(input_ids, max_length=100, do_sample=True, top_p=0.95, top_k=50)\n",
        "\n",
        "# Decode and print\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW47tjI-eAUN",
        "outputId": "6edfe6be-dc80-484a-eb49-0a45c28b6f89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "india economy, he says, could be \"more than just a technical phenomenon and a cultural issue\" for China.\"\n",
            "\n",
            "\"I do not think it's inevitable,\" he adds. \"It could be just like what happened in the world of the 1980s or in the case of China's industrialisation. But it's definitely not the result of a lack of infrastructure or industrial capacity, or not all of that and that. It could also be because of political pressure from the West or because\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "text_gen = generator(\"About indian cricket team,\", max_length=30, num_return_sequences=5)\n",
        "print(text_gen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oo7j_0Pj5F6",
        "outputId": "cf5ea573-9d96-4568-d912-407ab71a9453"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"About indian cricket team, for example, in this week's match against Pakistan, when the Indians came for what was a 3-1 win, a game that was played out in Pakistan's own stadium, at the same time the home team's cricket team was playing in a cricket stadium, at a stadium in Pakistan's capital, Islamabad. It was a match that was played for the first time, and not the first time, in Pakistan's capital, Islamabad. It was there that we learnt the lessons that we had in Pakistan, and in other countries, but it was also a match that we learned in Pakistan. It was a game that we learned in Pakistan, in other countries that we used to take for granted. We used to give it to others, to say, 'Let the young guys play cricket and let the old guys play cricket.' In Pakistan, we would give them the same amount of money as we gave them in our cricket programs, but we would not give them the same money as we gave them in our cricket programs, so it was a game of trust and respect.\\n\\nCricket.com: How do you feel about the recent Pakistan cricket ban, and why do you think that is a real issue?\\n\\nDakota: I feel very, very\"}, {'generated_text': \"About indian cricket team, the team had won the title in the past but had not won a trophy in the past 12 months.\\n\\nThe team's victory came to a sudden end on Friday night when the team's captain was caught on camera beating his men on a field and was given a penalty.\\n\\nThe team had won the title in the past but had not won a trophy in the past 12 months\\n\\nThe captain of the Indian cricket team, the team had won the title in the past but had not won a trophy in the past 12 months\\n\\nThe captain's team, the team had won the title in the past but had not won a trophy in the past 12 months\\n\\nThe team was on the verge of a loss in the final when a ball sailed over the back of the net and left the team with a score of 3-2.\\n\\nThe team was on the verge of a loss in the final when a ball sailed over the back of the net and left the team with a score of 3-2.\\n\\nThe captain was caught on camera beating his men on a field and given a penalty before the game was over.\\n\\n'After the game, I think the captain told me that he had seen his team lose, and I was like\"}, {'generated_text': \"About indian cricket team, who are currently playing against the best of the best in Siachen, Sarmatian and Nuremberg.\\n\\nWhat is the future for India in South Africa?\\n\\nIndia are in the early stages of a successful turnaround in their cricketing career. The last time they played in a World Cup (2009-10) was in 2009 (they were only 2-0 up in the first innings).\\n\\nHowever, they have already made some big signings from the likes of Aditya bhai, Virat Kohli, Virender Sehwag, Gautam Gambhir and Aiyar Ali. These signings have helped improve the game in South Africa's side and have included a number of former players such as Sri Lanka's Srinivasan, Pakistan's Mohan Bhagwat and former England and New Zealand cricketers such as Ashwin Ramachandran.\\n\\nIndia have been playing at an extremely high level for the last few years and have been able to build on their success in South Africa. The hope is that they can stay ahead of their rivals in the Asian Cup qualifiers against Zimbabwe, Bangladesh and Sri Lanka in May to prove their strength.\\n\\nWhat do you think of the future?\\n\\nI\"}, {'generated_text': 'About indian cricket team, he was a hero. He saved lives. He led people to a place of safety. He had a great team. He was the hero of the country. He was the hero of India. He was the hero of Pakistan. He was the hero of the city. And he was the hero of the country. And he was the hero of the country. And he was the hero of the city. And he was the hero of the country. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the hero of the city. And he was the'}, {'generated_text': \"About indian cricket team, who had been in India for 13 years.\\n\\nThe match between Hyderabad's Vindakkad and Mumbai's Chhatrapati Shivaji pitted the two sides against each other for the third time.\\n\\nIn the third Test between the two sides, the two sides lost a commanding 4-2.\\n\\nIn the fourth Test, the Hyderabad side won the game 6-6.\\n\\nThe two sides met again in the second Test, where the Hyderabad side lost 4-2.\\n\\nIndia had been hoping for a win in the next three Tests with the Mumbai side winning 6-6.\\n\\nThe last Test between the two sides had been in Mumbai in September.\\n\\nThe two sides had been together for just over three months before the game against the two sides.\\n\\nHowever, the match against the two sides would ultimately be decided by the two sides.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"about indian economy.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffc-VOl4j5I_",
        "outputId": "f9e08230-97b3-4cc9-c1e6-5a34410866dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0863,  0.0456, -0.5781,  ..., -0.1313,  0.0304, -0.1117],\n",
            "         [ 0.0102, -0.4237, -0.1496,  ..., -0.3787, -0.2636,  0.6461],\n",
            "         [ 0.3847, -0.1299, -0.8045,  ...,  0.1107,  0.0851,  0.1513],\n",
            "         [ 0.0122, -0.0766, -0.6437,  ...,  0.2880,  0.1527, -0.0295],\n",
            "         [ 0.2205, -0.2161, -0.1846,  ...,  0.0231,  0.0182,  0.1578]]],\n",
            "       grad_fn=<ViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7tlAi8oj5Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-NzlDEzj5QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVXtUw2SeAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9IYOnjqteAaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhpHrCCBeAc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnUhXhJ5eAfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHL5nRn6eAiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}